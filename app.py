"""
Interface SIMPLE - Florence2 + Qwen + GraphRAG
TH√àME FERTILIT√â ROSE/FLORAL üå∏
"""

import gradio as gr
import torch
import gc
from pathlib import Path
from PIL import Image

from stt import SpeechToText
from tts import TextToSpeech


class SimpleMedicalAssistant:
    """Version optimis√©e - Th√®me Fertilit√©"""
    
    def __init__(self, models_dir="/content/drive/MyDrive/fertility_models/saved_models"):
        print("üöÄ Initialisation...")
        self.models_dir = Path(models_dir)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Modules l√©gers (toujours en m√©moire)
        print("  üìù STT...")
        self.stt = SpeechToText(model_type="whisper", model_size="base", language="en", device="cpu")
        
        print("  üîä TTS...")
        self.tts = TextToSpeech(backend="gtts", language="en", output_dir="./audio_responses")
        
        print("  üìö GraphRAG...")
        self._load_graphrag()
        
        print("\n‚úÖ Pr√™t! (Florence2 et Qwen se chargent √† la demande)")
    
    def _load_graphrag(self):
        """Charge GraphRAG"""
        try:
            chroma_path = self.models_dir / "chroma_db"
            from langchain_community.embeddings import HuggingFaceEmbeddings
            from langchain_community.vectorstores import Chroma
            
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'}
            )
            
            self.vector_store = Chroma(
                persist_directory=str(chroma_path),
                embedding_function=embeddings,
                collection_name="fertility_collection"
            )
            print(f"     ‚úÖ GraphRAG charg√©")
        except Exception as e:
            print(f"     ‚ö†Ô∏è GraphRAG erreur: {e}")
            self.vector_store = None
    
    def analyze_image_with_florence(self, image_path):
        """Charge Florence2 ‚Üí Analyse ‚Üí Lib√®re"""
        print("\nüîÑ Chargement Florence2...")
        
        try:
            from transformers import AutoModelForCausalLM, AutoProcessor
            
            vlm_path = self.models_dir / "florence2"
            
            model = AutoModelForCausalLM.from_pretrained(
                vlm_path / "model",
                trust_remote_code=True,
                torch_dtype=torch.float16,
                attn_implementation="eager"
            ).to(self.device)
            
            processor = AutoProcessor.from_pretrained(
                vlm_path / "processor",
                trust_remote_code=True
            )
            
            print("‚úÖ Florence2 charg√©")
            
            img = Image.open(image_path).convert('RGB')
            print(f"‚úÖ Image charg√©e: {img.size}")
            
            inputs = processor(
                text="<OCR_WITH_REGION>", 
                images=img, 
                return_tensors="pt"
            )
            
            if inputs is None:
                raise ValueError("Le processor a retourn√© None")
            
            if 'pixel_values' not in inputs:
                raise ValueError("pixel_values manquant dans inputs")
                
            if inputs['pixel_values'] is None:
                raise ValueError("pixel_values est None")
            
            print(f"‚úÖ Inputs pr√©par√©s: {list(inputs.keys())}")
            
            inputs = {
                k: v.to(self.device, dtype=torch.float16) if v.dtype == torch.float32 else v.to(self.device) 
                for k, v in inputs.items()
            }
            
            print("üîÑ Analyse de l'image...")
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    num_beams=3,
                    do_sample=False,
                    use_cache=False
                )
            
            if generated_ids is None:
                raise ValueError("model.generate() a retourn√© None")
            
            if not isinstance(generated_ids, torch.Tensor):
                raise ValueError(f"generated_ids n'est pas un Tensor, type: {type(generated_ids)}")
            
            if generated_ids.numel() == 0:
                raise ValueError("generated_ids est un tensor vide")
            
            print(f"‚úÖ G√©n√©ration OK, shape: {generated_ids.shape}")
            
            text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            result = text.replace("<OCR_WITH_REGION>", "").strip()
            
            if not result:
                result = "‚ö†Ô∏è Aucun texte d√©tect√© dans l'image"
            
            print(f"‚úÖ Analyse termin√©e: {len(result)} caract√®res")
            
            print("üßπ Nettoyage m√©moire...")
            del model, processor, inputs, generated_ids, img
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            return result
            
        except Exception as e:
            print(f"‚ùå ERREUR: {str(e)}")
            import traceback
            traceback.print_exc()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            return f"‚ùå Impossible d'analyser l'image: {str(e)}"
    
    def generate_with_qwen(self, query, image_context=""):
        """Charge Qwen ‚Üí G√©n√®re ‚Üí Lib√®re"""
        print("\nüîÑ Chargement Qwen...")
        
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            
            llm_path = self.models_dir / "qwen"
            
            model = AutoModelForCausalLM.from_pretrained(
                llm_path / "model",
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            ).to(self.device)
            
            tokenizer = AutoTokenizer.from_pretrained(
                llm_path / "tokenizer",
                trust_remote_code=True
            )
            
            print("‚úÖ Qwen charg√©, g√©n√©ration en cours...")
            
            context = ""
            if self.vector_store:
                try:
                    docs = self.vector_store.similarity_search(query, k=3)
                    context = "\n\n".join([doc.page_content[:400] for doc in docs])
                except:
                    context = "Context unavailable"
            
            prompt = f"""You are a compassionate fertility assistant.

MEDICAL CONTEXT:
{context}

{"IMAGE ANALYSIS:" if image_context else ""}
{image_context}

INSTRUCTIONS:
- Be warm, empathetic, and clear
- Ground response in medical context
- Include disclaimer at end
- Never give definitive diagnosis

USER QUERY: {query}

RESPONSE:"""
            
            messages = [
                {"role": "system", "content": "You are a helpful medical assistant."},
                {"role": "user", "content": prompt}
            ]
            
            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=4096).to(self.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            print("üßπ Lib√©ration Qwen...")
            del model, tokenizer, inputs, outputs
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            return response.strip()
            
        except Exception as e:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            return f"‚ùå Erreur Qwen: {str(e)}"
    
    def process_query(self, text_input, audio_input, image_input):
        """Handler principal"""
        
        question = ""
        image_context = ""
        
        if audio_input is not None:
            print("\nüì¢ Mode: Audio")
            stt_result = self.stt.transcribe_file(audio_input)
            if stt_result["success"]:
                question = stt_result["text"]
            else:
                return f"‚ùå Erreur STT: {stt_result['error']}", None
        
        elif text_input and text_input.strip():
            print("\nüìù Mode: Texte")
            question = text_input.strip()
        
        else:
            return "‚ùå Veuillez entrer une question (texte ou audio)", None
        
        if image_input is not None:
            print("\nüì∏ D√©tection d'image, analyse...")
            image_context = self.analyze_image_with_florence(image_input)
        
        print(f"\nüí≠ Question: {question}")
        answer = self.generate_with_qwen(question, image_context)
        
        result_text = f"""## üìù Question:
> {question}

{"## üì∏ Image analys√©e:" if image_context else ""}
{"```" + image_context[:200] + "...```" if image_context else ""}

## ü§ñ R√©ponse:

{answer}

---
*‚öïÔ∏è Disclaimer: Cette information est √©ducative. Consultez toujours un professionnel de sant√©.*
"""
        
        print("\nüîä G√©n√©ration audio...")
        tts_result = self.tts.synthesize(answer)
        audio_output = tts_result["output_file"] if tts_result["success"] else None
        
        return result_text, audio_output
    
    def create_interface(self):
        """Interface TH√àME FERTILIT√â ROSE üå∏"""
        
        with gr.Blocks(
            title="Assistant Fertilit√© üå∏", 
            theme=gr.themes.Soft(
                primary_hue="pink",
                secondary_hue="rose",
            ),
            css="""
                .header-title {
                    text-align: center;
                    margin-bottom: 15px;
                    background: linear-gradient(135deg, #ffc0cb 0%, #ffb6c1 100%);
                    padding: 25px;
                    border-radius: 20px;
                    box-shadow: 0 4px 8px rgba(255, 182, 193, 0.4);
                }
                .input-section {
                    background: linear-gradient(135deg, #ffb6c1 0%, #ffc0cb 50%, #ffb3d9 100%);
                    padding: 15px;
                    border-radius: 15px;
                    margin-top: 10px;
                    box-shadow: 0 4px 6px rgba(255, 182, 193, 0.3);
                }
                .chat-container {
                    max-height: 350px;
                    overflow-y: auto;
                    padding: 15px;
                    border: 2px solid #ffb6c1;
                    border-radius: 15px;
                    background: linear-gradient(to bottom, #fff5f7 0%, #ffe4e9 100%);
                    margin-bottom: 15px;
                }
            """
        ) as demo:
            
            # HEADER ROSE
            with gr.Row():
                gr.Markdown(
                    """
# üå∏ Assistant Fertilit√© üå∏
### *Votre compagnon bienveillant pour votre parcours* üíï
                    """,
                    elem_classes="header-title"
                )
            
            # CONVERSATION
            gr.Markdown("### üå∫ Conversation")
            
            with gr.Group(elem_classes="chat-container"):
                chatbot = gr.Chatbot(
                    value=[],
                    label="",
                    height=150,  
                    show_label=False,
                    bubble_full_width=False
                )
            
            with gr.Row():
                output_audio = gr.Audio(
                    label="üéµ √âcouter la r√©ponse",
                    visible=True
                )
            
            # ZONE INPUT ROSE
            with gr.Group(elem_classes="input-section"):
                gr.Markdown("### üå∑ Posez votre question")
                
                with gr.Row():
                    text_input = gr.Textbox(
                        label="",
                        placeholder="üí¨ √âcrivez votre question ici...",
                        lines=1,
                        max_lines=2,
                        scale=4,
                        container=False
                    )
                
                with gr.Row():
                    audio_input = gr.Audio(
                        sources=["microphone", "upload"],
                        type="filepath",
                        label="üé§",
                        scale=1,
                        container=False
                    )
                    
                    image_input = gr.Image(
                        type="filepath",
                        label="üìã",
                        scale=1,
                        container=False,
                        height=100
                    )
                
                submit_btn = gr.Button(
                    "üå∏ Envoyer",
                    variant="primary",
                    size="sm"
                )
            
            chat_history = gr.State([])
            
            def chat_interface(text_input, audio_input, image_input, history):
                result_text, audio_file = self.process_query(text_input, audio_input, image_input)
                
                if "## üìù Question:" in result_text:
                    parts = result_text.split("## ü§ñ R√©ponse:")
                    question_part = parts[0].replace("## üìù Question:", "").strip()
                    question = question_part.split("\n")[0].replace(">", "").strip()
                    
                    if len(parts) > 1:
                        response = parts[1].split("---")[0].strip()
                    else:
                        response = "Erreur lors de la g√©n√©ration de la r√©ponse."
                else:
                    question = text_input if text_input else "Question audio"
                    response = result_text
                
                history.append([question, response])
                
                return history, "", None, None, audio_file
            
            submit_btn.click(
                fn=chat_interface,
                inputs=[text_input, audio_input, image_input, chat_history],
                outputs=[chatbot, text_input, audio_input, image_input, output_audio]
            )
        
        return demo
