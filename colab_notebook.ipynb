{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "318dcec31dd54cc18ecbffdfdc0e0b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1929fd83dd8c4fe68a6d9c58590df96e",
              "IPY_MODEL_d5cbfa306bbd4cf3a084b3b8a6a6b77c",
              "IPY_MODEL_95e943b3e0b04aaaa3829539c83d7375"
            ],
            "layout": "IPY_MODEL_c3d277b2a0c144778b551d526943e85b"
          }
        },
        "1929fd83dd8c4fe68a6d9c58590df96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97ecf4c462f422b853fec9720376ecf",
            "placeholder": "​",
            "style": "IPY_MODEL_0311fd9563e14aeabb2ec1ca8733f5cd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d5cbfa306bbd4cf3a084b3b8a6a6b77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8905244ce8124cd3afeafa8c6b92354e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbe01e6ce91342ac9624291de9cd2311",
            "value": 2
          }
        },
        "95e943b3e0b04aaaa3829539c83d7375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e472eaf0551423fa9358361051f25f2",
            "placeholder": "​",
            "style": "IPY_MODEL_a080da3fa4e340dc96800b9f9eb9cd48",
            "value": " 2/2 [01:12&lt;00:00, 33.83s/it]"
          }
        },
        "c3d277b2a0c144778b551d526943e85b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97ecf4c462f422b853fec9720376ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0311fd9563e14aeabb2ec1ca8733f5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8905244ce8124cd3afeafa8c6b92354e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe01e6ce91342ac9624291de9cd2311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e472eaf0551423fa9358361051f25f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a080da3fa4e340dc96800b9f9eb9cd48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58f91a9940f648f6a8fb8c368a09e355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8736518280041ccbdd46354eba117dd",
              "IPY_MODEL_3b89c3f9f3c7479b972595b7ceaaad5c",
              "IPY_MODEL_62bd0dde50df44cca8435b57ff48fc54"
            ],
            "layout": "IPY_MODEL_c631f571d409411da4e137cbda9ab4cf"
          }
        },
        "a8736518280041ccbdd46354eba117dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_830c160353b84f77a0468d67468d03c8",
            "placeholder": "​",
            "style": "IPY_MODEL_e35f8fa08bd147899fb4f48f84bc6be3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3b89c3f9f3c7479b972595b7ceaaad5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78982ea77bdc424ea82b4d4a1aeca85b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5076830c79c3438d9a775ea9f53f7ac6",
            "value": 2
          }
        },
        "62bd0dde50df44cca8435b57ff48fc54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d66b962941e34e30a377914344ee644d",
            "placeholder": "​",
            "style": "IPY_MODEL_ea70c08530cc47babfc6bf178f97c2f5",
            "value": " 2/2 [00:01&lt;00:00,  1.96it/s]"
          }
        },
        "c631f571d409411da4e137cbda9ab4cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830c160353b84f77a0468d67468d03c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35f8fa08bd147899fb4f48f84bc6be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78982ea77bdc424ea82b4d4a1aeca85b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5076830c79c3438d9a775ea9f53f7ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d66b962941e34e30a377914344ee644d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea70c08530cc47babfc6bf178f97c2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bae362ecebe447009177812f0b090e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22a11aa273a641f5b2c252e08d14cbe2",
              "IPY_MODEL_a9f60d3344d44012a1507f670ebeb7ea",
              "IPY_MODEL_2d3cd0a172fe4daea4236818f67d9ada"
            ],
            "layout": "IPY_MODEL_33ba37518c2b4055a8400d28d2f493e6"
          }
        },
        "22a11aa273a641f5b2c252e08d14cbe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ebe6e0397c346eb852463cf21dd84e5",
            "placeholder": "​",
            "style": "IPY_MODEL_f643e4e2f35e42fdb16d7339aa19567a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a9f60d3344d44012a1507f670ebeb7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7faa97e947644b309261261e00c9dc66",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bc74920fc0649ad9b3b319059ab89de",
            "value": 2
          }
        },
        "2d3cd0a172fe4daea4236818f67d9ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f910f8dcdd04aa1bfb316054ad9546f",
            "placeholder": "​",
            "style": "IPY_MODEL_bfb98f805954402a893d298b7ac49f9e",
            "value": " 2/2 [00:00&lt;00:00,  2.66it/s]"
          }
        },
        "33ba37518c2b4055a8400d28d2f493e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ebe6e0397c346eb852463cf21dd84e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f643e4e2f35e42fdb16d7339aa19567a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7faa97e947644b309261261e00c9dc66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bc74920fc0649ad9b3b319059ab89de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f910f8dcdd04aa1bfb316054ad9546f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfb98f805954402a893d298b7ac49f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbb24d4fec0d43a19679c168a54ae7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44790d4dbd064d5bb1d56ddb48e07eea",
              "IPY_MODEL_f13f0d3834524657945a4e0ea231ad8d",
              "IPY_MODEL_0f6a21c9404c477fbdc7c33d2daf82f9"
            ],
            "layout": "IPY_MODEL_321aa05486614b5d8727136a7ed65d58"
          }
        },
        "44790d4dbd064d5bb1d56ddb48e07eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11f18864eb5746ec94ec2ed296c1e307",
            "placeholder": "​",
            "style": "IPY_MODEL_b767abececb74f44813096dd65d45f7a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f13f0d3834524657945a4e0ea231ad8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a7650b1f424319990e655dbb08da31",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c32291a3c194b0198256a3d80bbd86a",
            "value": 2
          }
        },
        "0f6a21c9404c477fbdc7c33d2daf82f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1676e51aad684e70b691651de554029f",
            "placeholder": "​",
            "style": "IPY_MODEL_d940fb2f8ddc43b28b149b327990dc30",
            "value": " 2/2 [00:00&lt;00:00,  2.26it/s]"
          }
        },
        "321aa05486614b5d8727136a7ed65d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f18864eb5746ec94ec2ed296c1e307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b767abececb74f44813096dd65d45f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0a7650b1f424319990e655dbb08da31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c32291a3c194b0198256a3d80bbd86a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1676e51aad684e70b691651de554029f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d940fb2f8ddc43b28b149b327990dc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(\"📦 Installation des dépendances système...\")\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq portaudio19-dev python3-pyaudio\n",
        "print(\"✅ Dépendances système installées\\n\")\n",
        "\n",
        "print(\"📦 Installation des packages Python...\")\n",
        "!pip install -q pyaudio sounddevice\n",
        "!pip install -q gradio\n",
        "!pip install -q openai-whisper\n",
        "!pip install -q gtts\n",
        "!pip install -q scipy\n",
        "!pip install -q transformers\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install -q langchain langchain-community\n",
        "!pip install -q chromadb\n",
        "!pip install -q pillow\n",
        "\n",
        "print(\"✅ Tous les packages sont installés!\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yTQ6XqT-2z94",
        "outputId": "61780205-5b95-49a4-a94c-1a42faff28ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installation des dépendances système...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "dpkg: libjack-jackd2-0:amd64: dependency problems, but removing anyway as you requested:\n",
            " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\n",
            "  Package libjack-jackd2-0:amd64 is to be removed.\n",
            "  Package libjack-0.125 is not installed.\n",
            "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\n",
            " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\n",
            "  Package libjack-jackd2-0:amd64 is to be removed.\n",
            "  Package libjack-0.125 is not installed.\n",
            "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\n",
            "\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Removing libjack-jackd2-0:amd64 (1.9.20~dfsg-1) ...\n",
            "Selecting previously unselected package libjack0:amd64.\n",
            "(Reading database ... 121703 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libjack0_1%3a0.125.0-3build2_amd64.deb ...\n",
            "Unpacking libjack0:amd64 (1:0.125.0-3build2) ...\n",
            "Selecting previously unselected package libasound2-dev:amd64.\n",
            "Preparing to unpack .../1-libasound2-dev_1.2.6.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libasound2-dev:amd64 (1.2.6.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libjack-dev.\n",
            "Preparing to unpack .../2-libjack-dev_1%3a0.125.0-3build2_amd64.deb ...\n",
            "Unpacking libjack-dev (1:0.125.0-3build2) ...\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "Preparing to unpack .../3-libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../4-libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../5-portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package python3-pyaudio.\n",
            "Preparing to unpack .../6-python3-pyaudio_0.2.11-1.3ubuntu1_amd64.deb ...\n",
            "Unpacking python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Setting up libjack0:amd64 (1:0.125.0-3build2) ...\n",
            "Setting up libjack-dev (1:0.125.0-3build2) ...\n",
            "Setting up libasound2-dev:amd64 (1.2.6.1-1ubuntu1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "✅ Dépendances système installées\n",
            "\n",
            "📦 Installation des packages Python...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Tous les packages sont installés!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4mN8MoNVtTq",
        "outputId": "d33e6461-64d3-449b-f8a6-66963373a283"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q openai-whisper\n",
        "!pip install -q gtts\n",
        "!pip install -q scipy\n",
        "!pip install -q transformers\n",
        "\n",
        "print(\"✅ Toutes les dépendances sont installées!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mELA89o5yRN3",
        "outputId": "542836db-9013-4d9f-d508-eee745ad60fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Toutes les dépendances sont installées!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test des imports\n",
        "\"\"\"\n",
        "\n",
        "print(\"🧪 Test des imports...\\n\")\n",
        "\n",
        "# Test STT\n",
        "try:\n",
        "    from stt import SpeechToText\n",
        "    print(\"✅ STT importé avec succès\")\n",
        "    stt = SpeechToText(model_type=\"whisper\", model_size=\"base\", language=\"en\", device=\"cpu\")\n",
        "    print(\"   ✅ STT initialisé\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur STT: {e}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test TTS\n",
        "try:\n",
        "    from tts import TextToSpeech\n",
        "    print(\"✅ TTS importé avec succès\")\n",
        "    tts = TextToSpeech(backend=\"gtts\", language=\"en\")\n",
        "    print(\"   ✅ TTS initialisé\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur TTS: {e}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test GraphRAG\n",
        "try:\n",
        "    from graphrag_index import FertilityGraphRAG\n",
        "    print(\"✅ FertilityGraphRAG importé avec succès\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erreur GraphRAG: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WohkVZF93JWI",
        "outputId": "154251dd-f8dc-4534-861b-aab862c3989e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Test des imports...\n",
            "\n",
            "✅ STT importé avec succès\n",
            "Initializing STT Module | Backend: whisper, Model: base, Language: en, Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 249MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whisper model loaded successfully\n",
            "   ✅ STT initialisé\n",
            "\n",
            "✅ TTS importé avec succès\n",
            " gTTS prêt\n",
            "   ✅ TTS initialisé\n",
            "\n",
            "✅ FertilityGraphRAG importé avec succès\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "models_path = '/content/drive/MyDrive/fertility_models/fertility_models/saved_models'\n",
        "\n",
        "print(\"📂 Structure de vos modèles:\\n\")\n",
        "\n",
        "# Lire info.json\n",
        "info_path = os.path.join(models_path, 'info.json')\n",
        "if os.path.exists(info_path):\n",
        "    with open(info_path, 'r') as f:\n",
        "        info = json.load(f)\n",
        "    print(\"📄 info.json:\")\n",
        "    print(json.dumps(info, indent=2))\n",
        "    print()\n",
        "\n",
        "# Vérifier chaque dossier\n",
        "folders = {\n",
        "    'chroma_db': 'Base de données vectorielle',\n",
        "    'florence2': 'Modèle de vision',\n",
        "    'qwen': 'Modèle de langage'\n",
        "}\n",
        "\n",
        "for folder, description in folders.items():\n",
        "    folder_path = os.path.join(models_path, folder)\n",
        "    if os.path.exists(folder_path):\n",
        "        files = os.listdir(folder_path)\n",
        "        size = sum(os.path.getsize(os.path.join(folder_path, f))\n",
        "                   for f in files if os.path.isfile(os.path.join(folder_path, f)))\n",
        "        print(f\"✅ {folder}/ - {description}\")\n",
        "        print(f\"   Fichiers: {len(files)}, Taille: {size/1024/1024:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"❌ {folder}/ - Manquant\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOLAW_m4y8hK",
        "outputId": "f72f3c87-5154-4805-ee15-ac8660b809ad",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Structure de vos modèles:\n",
            "\n",
            "📄 info.json:\n",
            "{\n",
            "  \"vlm_model\": \"microsoft/Florence-2-base\",\n",
            "  \"llm_model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
            "  \"has_graphrag\": true\n",
            "}\n",
            "\n",
            "✅ chroma_db/ - Base de données vectorielle\n",
            "   Fichiers: 1, Taille: 0.16 MB\n",
            "✅ florence2/ - Modèle de vision\n",
            "   Fichiers: 2, Taille: 0.00 MB\n",
            "✅ qwen/ - Modèle de langage\n",
            "   Fichiers: 2, Taille: 0.00 MB\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "models_path = '/content/drive/MyDrive/fertility_models/fertility_models/saved_models'\n",
        "\n",
        "print(\"🔍 DIAGNOSTIC COMPLET - Structure Qwen\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Vérifier le dossier qwen\n",
        "qwen_path = os.path.join(models_path, 'qwen')\n",
        "\n",
        "if os.path.exists(qwen_path):\n",
        "    print(f\"\\n✅ Dossier qwen trouvé: {qwen_path}\\n\")\n",
        "\n",
        "    # Lister TOUS les fichiers et sous-dossiers\n",
        "    print(\"📁 Contenu du dossier qwen:\")\n",
        "    for root, dirs, files in os.walk(qwen_path):\n",
        "        level = root.replace(qwen_path, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:20]:  # Limiter à 20 fichiers par dossier\n",
        "            size = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n",
        "            print(f'{subindent}├── {file} ({size:.2f} MB)')\n",
        "        if len(files) > 20:\n",
        "            print(f'{subindent}└── ... et {len(files)-20} autres fichiers')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\\n🔍 Recherche des fichiers essentiels:\\n\")\n",
        "\n",
        "    # Chercher config.json\n",
        "    config_files = []\n",
        "    for root, dirs, files in os.walk(qwen_path):\n",
        "        if 'config.json' in files:\n",
        "            config_files.append(root)\n",
        "\n",
        "    if config_files:\n",
        "        print(f\"✅ config.json trouvé dans:\")\n",
        "        for path in config_files:\n",
        "            print(f\"   {path}\")\n",
        "            # Vérifier les fichiers à côté\n",
        "            files_in_dir = os.listdir(path)\n",
        "            print(f\"   Fichiers à côté: {', '.join(files_in_dir[:10])}\")\n",
        "    else:\n",
        "        print(\"❌ config.json NON TROUVÉ\")\n",
        "\n",
        "    # Chercher les fichiers .bin ou .safetensors\n",
        "    print(\"\\n🔍 Fichiers de poids du modèle:\")\n",
        "    weight_files = []\n",
        "    for root, dirs, files in os.walk(qwen_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.bin', '.safetensors', '.pt', '.pth')):\n",
        "                weight_files.append(os.path.join(root, file))\n",
        "\n",
        "    if weight_files:\n",
        "        print(f\"✅ Trouvé {len(weight_files)} fichier(s) de poids:\")\n",
        "        for wf in weight_files[:5]:\n",
        "            size = os.path.getsize(wf) / (1024*1024)\n",
        "            print(f\"   {wf} ({size:.2f} MB)\")\n",
        "    else:\n",
        "        print(\"❌ Aucun fichier de poids trouvé\")\n",
        "\n",
        "    # Chercher tokenizer\n",
        "    print(\"\\n🔍 Fichiers du tokenizer:\")\n",
        "    tokenizer_files = []\n",
        "    for root, dirs, files in os.walk(qwen_path):\n",
        "        for file in files:\n",
        "            if 'tokenizer' in file.lower():\n",
        "                tokenizer_files.append(os.path.join(root, file))\n",
        "\n",
        "    if tokenizer_files:\n",
        "        print(f\"✅ Trouvé {len(tokenizer_files)} fichier(s) tokenizer:\")\n",
        "        for tf in tokenizer_files[:5]:\n",
        "            print(f\"   {tf}\")\n",
        "    else:\n",
        "        print(\"❌ Aucun fichier tokenizer trouvé\")\n",
        "\n",
        "else:\n",
        "    print(f\"❌ Dossier qwen NON TROUVÉ: {qwen_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\n💡 CONCLUSION:\")\n",
        "print(\"Le chemin correct à utiliser dans from_pretrained() est celui\")\n",
        "print(\"qui contient directement config.json\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "dKLhugLcZ-nl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Interface SIMPLE - Florence2 + Qwen + GraphRAG\n",
        "THÈME FERTILITÉ ROSE/FLORAL 🌸\n",
        "\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "from stt import SpeechToText\n",
        "from tts import TextToSpeech\n",
        "\n",
        "\n",
        "class SimpleMedicalAssistant:\n",
        "    \"\"\"Version optimisée - Thème Fertilité\"\"\"\n",
        "\n",
        "    def __init__(self, models_dir=\"/content/drive/MyDrive/fertility_models/fertility_models/saved_models\"):\n",
        "        print(\"🚀 Initialisation...\")\n",
        "        self.models_dir = Path(models_dir)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Modules légers (toujours en mémoire)\n",
        "        print(\"  📝 STT...\")\n",
        "        self.stt = SpeechToText(model_type=\"whisper\", model_size=\"base\", language=\"en\", device=\"cpu\")\n",
        "\n",
        "        print(\"  🔊 TTS...\")\n",
        "        self.tts = TextToSpeech(backend=\"gtts\", language=\"en\", output_dir=\"./audio_responses\")\n",
        "\n",
        "        print(\"  📚 GraphRAG...\")\n",
        "        self._load_graphrag()\n",
        "\n",
        "        print(\"\\n✅ Prêt! (Florence2 et Qwen se chargent à la demande)\")\n",
        "\n",
        "    def _load_graphrag(self):\n",
        "        \"\"\"Charge GraphRAG\"\"\"\n",
        "        try:\n",
        "            chroma_path = self.models_dir / \"chroma_db\"\n",
        "            from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "            from langchain_community.vectorstores import Chroma\n",
        "\n",
        "            embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                model_kwargs={'device': 'cpu'}\n",
        "            )\n",
        "\n",
        "            self.vector_store = Chroma(\n",
        "                persist_directory=str(chroma_path),\n",
        "                embedding_function=embeddings,\n",
        "                collection_name=\"fertility_collection\"\n",
        "            )\n",
        "            print(f\"     ✅ GraphRAG chargé\")\n",
        "        except Exception as e:\n",
        "            print(f\"     ⚠️ GraphRAG erreur: {e}\")\n",
        "            self.vector_store = None\n",
        "\n",
        "    def analyze_image_with_florence(self, image_path):\n",
        "        \"\"\"Charge Florence2 → Analyse → Libère\"\"\"\n",
        "        print(\"\\n🔄 Chargement Florence2...\")\n",
        "\n",
        "        try:\n",
        "            from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "            # ✅ Chemin vers florence2/model et florence2/processor (ou direct si pas de sous-dossiers)\n",
        "            vlm_base = self.models_dir / \"florence2\"\n",
        "\n",
        "            # Vérifier si model/ et processor/ existent\n",
        "            if (vlm_base / \"model\").exists():\n",
        "                model_path = str(vlm_base / \"model\")\n",
        "                processor_path = str(vlm_base / \"processor\")\n",
        "            else:\n",
        "                # Sinon, utiliser le dossier principal\n",
        "                model_path = str(vlm_base)\n",
        "                processor_path = str(vlm_base)\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=torch.float16,\n",
        "                attn_implementation=\"eager\",\n",
        "                local_files_only=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            processor = AutoProcessor.from_pretrained(\n",
        "                processor_path,\n",
        "                trust_remote_code=True,\n",
        "                local_files_only=True\n",
        "            )\n",
        "\n",
        "            print(\"✅ Florence2 chargé\")\n",
        "\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "            print(f\"✅ Image chargée: {img.size}\")\n",
        "\n",
        "            inputs = processor(\n",
        "                text=\"<OCR_WITH_REGION>\",\n",
        "                images=img,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            if inputs is None:\n",
        "                raise ValueError(\"Le processor a retourné None\")\n",
        "\n",
        "            if 'pixel_values' not in inputs:\n",
        "                raise ValueError(\"pixel_values manquant dans inputs\")\n",
        "\n",
        "            if inputs['pixel_values'] is None:\n",
        "                raise ValueError(\"pixel_values est None\")\n",
        "\n",
        "            print(f\"✅ Inputs préparés: {list(inputs.keys())}\")\n",
        "\n",
        "            inputs = {\n",
        "                k: v.to(self.device, dtype=torch.float16) if v.dtype == torch.float32 else v.to(self.device)\n",
        "                for k, v in inputs.items()\n",
        "            }\n",
        "\n",
        "            print(\"🔄 Analyse de l'image...\")\n",
        "            with torch.no_grad():\n",
        "                generated_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=1024,\n",
        "                    num_beams=3,\n",
        "                    do_sample=False,\n",
        "                    use_cache=False\n",
        "                )\n",
        "\n",
        "            if generated_ids is None:\n",
        "                raise ValueError(\"model.generate() a retourné None\")\n",
        "\n",
        "            if not isinstance(generated_ids, torch.Tensor):\n",
        "                raise ValueError(f\"generated_ids n'est pas un Tensor, type: {type(generated_ids)}\")\n",
        "\n",
        "            if generated_ids.numel() == 0:\n",
        "                raise ValueError(\"generated_ids est un tensor vide\")\n",
        "\n",
        "            print(f\"✅ Génération OK, shape: {generated_ids.shape}\")\n",
        "\n",
        "            text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "            result = text.replace(\"<OCR_WITH_REGION>\", \"\").strip()\n",
        "\n",
        "            if not result:\n",
        "                result = \"⚠️ Aucun texte détecté dans l'image\"\n",
        "\n",
        "            print(f\"✅ Analyse terminée: {len(result)} caractères\")\n",
        "\n",
        "            print(\"🧹 Nettoyage mémoire...\")\n",
        "            del model, processor, inputs, generated_ids, img\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERREUR: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            return f\"❌ Impossible d'analyser l'image: {str(e)}\"\n",
        "\n",
        "    def generate_with_qwen(self, query, image_context=\"\"):\n",
        "        \"\"\"Charge Qwen → Génère → Libère\"\"\"\n",
        "        print(\"\\n🔄 Chargement Qwen...\")\n",
        "\n",
        "        try:\n",
        "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "            # ✅ CORRECTION: Chemins vers les sous-dossiers model et tokenizer\n",
        "            model_path = str(self.models_dir / \"qwen\" / \"model\")\n",
        "            tokenizer_path = str(self.models_dir / \"qwen\" / \"tokenizer\")\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                trust_remote_code=True,\n",
        "                local_files_only=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\n",
        "                tokenizer_path,\n",
        "                trust_remote_code=True,\n",
        "                local_files_only=True\n",
        "            )\n",
        "\n",
        "            print(\"✅ Qwen chargé, génération en cours...\")\n",
        "\n",
        "            context = \"\"\n",
        "            if self.vector_store:\n",
        "                try:\n",
        "                    docs = self.vector_store.similarity_search(query, k=3)\n",
        "                    context = \"\\n\\n\".join([doc.page_content[:400] for doc in docs])\n",
        "                except:\n",
        "                    context = \"Context unavailable\"\n",
        "\n",
        "            prompt = f\"\"\"You are a compassionate fertility assistant.\n",
        "\n",
        "MEDICAL CONTEXT:\n",
        "{context}\n",
        "\n",
        "{\"IMAGE ANALYSIS:\" if image_context else \"\"}\n",
        "{image_context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Be warm, empathetic, and clear\n",
        "- Ground response in medical context\n",
        "- Include disclaimer at end\n",
        "- Never give definitive diagnosis\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "RESPONSE:\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=4096).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "            print(\"🧹 Libération Qwen...\")\n",
        "            del model, tokenizer, inputs, outputs\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            return response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            return f\"❌ Erreur Qwen: {str(e)}\"\n",
        "\n",
        "    def process_query(self, text_input, audio_input, image_input):\n",
        "        \"\"\"Handler principal\"\"\"\n",
        "\n",
        "        question = \"\"\n",
        "        image_context = \"\"\n",
        "\n",
        "        if audio_input is not None:\n",
        "            print(\"\\n📢 Mode: Audio\")\n",
        "            stt_result = self.stt.transcribe_file(audio_input)\n",
        "            if stt_result[\"success\"]:\n",
        "                question = stt_result[\"text\"]\n",
        "            else:\n",
        "                return f\"❌ Erreur STT: {stt_result['error']}\", None\n",
        "\n",
        "        elif text_input and text_input.strip():\n",
        "            print(\"\\n📝 Mode: Texte\")\n",
        "            question = text_input.strip()\n",
        "\n",
        "        else:\n",
        "            return \"❌ Veuillez entrer une question (texte ou audio)\", None\n",
        "\n",
        "        if image_input is not None:\n",
        "            print(\"\\n📸 Détection d'image, analyse...\")\n",
        "            image_context = self.analyze_image_with_florence(image_input)\n",
        "\n",
        "        print(f\"\\n💭 Question: {question}\")\n",
        "        answer = self.generate_with_qwen(question, image_context)\n",
        "\n",
        "        result_text = f\"\"\"## 📝 Question:\n",
        "> {question}\n",
        "\n",
        "{\"## 📸 Image analysée:\" if image_context else \"\"}\n",
        "{\"```\" + image_context[:200] + \"...```\" if image_context else \"\"}\n",
        "\n",
        "## 🤖 Réponse:\n",
        "\n",
        "{answer}\n",
        "\n",
        "---\n",
        "*⚕️ Disclaimer: Cette information est éducative. Consultez toujours un professionnel de santé.*\n",
        "\"\"\"\n",
        "\n",
        "        print(\"\\n🔊 Génération audio...\")\n",
        "        tts_result = self.tts.synthesize(answer)\n",
        "        audio_output = tts_result[\"output_file\"] if tts_result[\"success\"] else None\n",
        "\n",
        "        return result_text, audio_output\n",
        "\n",
        "    def create_interface(self):\n",
        "        \"\"\"Interface THÈME FERTILITÉ ROSE 🌸\"\"\"\n",
        "\n",
        "        with gr.Blocks(\n",
        "            title=\"Assistant Fertilité 🌸\",\n",
        "            theme=gr.themes.Soft(\n",
        "                primary_hue=\"pink\",\n",
        "                secondary_hue=\"rose\",\n",
        "            ),\n",
        "            css=\"\"\"\n",
        "                .header-title {\n",
        "                    text-align: center;\n",
        "                    margin-bottom: 15px;\n",
        "                    background: linear-gradient(135deg, #ffc0cb 0%, #ffb6c1 100%);\n",
        "                    padding: 25px;\n",
        "                    border-radius: 20px;\n",
        "                    box-shadow: 0 4px 8px rgba(255, 182, 193, 0.4);\n",
        "                }\n",
        "                .input-section {\n",
        "                    background: linear-gradient(135deg, #ffb6c1 0%, #ffc0cb 50%, #ffb3d9 100%);\n",
        "                    padding: 15px;\n",
        "                    border-radius: 15px;\n",
        "                    margin-top: 10px;\n",
        "                    box-shadow: 0 4px 6px rgba(255, 182, 193, 0.3);\n",
        "                }\n",
        "                .chat-container {\n",
        "                    max-height: 350px;\n",
        "                    overflow-y: auto;\n",
        "                    padding: 15px;\n",
        "                    border: 2px solid #ffb6c1;\n",
        "                    border-radius: 15px;\n",
        "                    background: linear-gradient(to bottom, #fff5f7 0%, #ffe4e9 100%);\n",
        "                    margin-bottom: 15px;\n",
        "                }\n",
        "            \"\"\"\n",
        "        ) as demo:\n",
        "\n",
        "            # HEADER ROSE\n",
        "            with gr.Row():\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "# 🌸 Assistant Fertilité 🌸\n",
        "### *Votre compagnon bienveillant pour votre parcours* 💕\n",
        "                    \"\"\",\n",
        "                    elem_classes=\"header-title\"\n",
        "                )\n",
        "\n",
        "            # CONVERSATION\n",
        "            gr.Markdown(\"### 🌺 Conversation\")\n",
        "\n",
        "            with gr.Group(elem_classes=\"chat-container\"):\n",
        "                chatbot = gr.Chatbot(\n",
        "                    value=[],\n",
        "                    label=\"\",\n",
        "                    height=150,\n",
        "                    show_label=False,\n",
        "                    bubble_full_width=False\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                output_audio = gr.Audio(\n",
        "                    label=\"🎵 Écouter la réponse\",\n",
        "                    visible=True\n",
        "                )\n",
        "\n",
        "            # ZONE INPUT ROSE\n",
        "            with gr.Group(elem_classes=\"input-section\"):\n",
        "                gr.Markdown(\"### 🌷 Posez votre question\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    text_input = gr.Textbox(\n",
        "                        label=\"\",\n",
        "                        placeholder=\"💬 Écrivez votre question ici...\",\n",
        "                        lines=1,\n",
        "                        max_lines=2,\n",
        "                        scale=4,\n",
        "                        container=False\n",
        "                    )\n",
        "\n",
        "                with gr.Row():\n",
        "                    audio_input = gr.Audio(\n",
        "                        sources=[\"microphone\", \"upload\"],\n",
        "                        type=\"filepath\",\n",
        "                        label=\"🎤\",\n",
        "                        scale=1,\n",
        "                        container=False\n",
        "                    )\n",
        "\n",
        "                    image_input = gr.Image(\n",
        "                        type=\"filepath\",\n",
        "                        label=\"📋\",\n",
        "                        scale=1,\n",
        "                        container=False,\n",
        "                        height=100\n",
        "                    )\n",
        "\n",
        "                submit_btn = gr.Button(\n",
        "                    \"🌸 Envoyer\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"sm\"\n",
        "                )\n",
        "\n",
        "            chat_history = gr.State([])\n",
        "\n",
        "            def chat_interface(text_input, audio_input, image_input, history):\n",
        "                result_text, audio_file = self.process_query(text_input, audio_input, image_input)\n",
        "\n",
        "                if \"## 📝 Question:\" in result_text:\n",
        "                    parts = result_text.split(\"## 🤖 Réponse:\")\n",
        "                    question_part = parts[0].replace(\"## 📝 Question:\", \"\").strip()\n",
        "                    question = question_part.split(\"\\n\")[0].replace(\">\", \"\").strip()\n",
        "\n",
        "                    if len(parts) > 1:\n",
        "                        response = parts[1].split(\"---\")[0].strip()\n",
        "                    else:\n",
        "                        response = \"Erreur lors de la génération de la réponse.\"\n",
        "                else:\n",
        "                    question = text_input if text_input else \"Question audio\"\n",
        "                    response = result_text\n",
        "\n",
        "                history.append([question, response])\n",
        "\n",
        "                return history, \"\", None, None, audio_file\n",
        "\n",
        "            submit_btn.click(\n",
        "                fn=chat_interface,\n",
        "                inputs=[text_input, audio_input, image_input, chat_history],\n",
        "                outputs=[chatbot, text_input, audio_input, image_input, output_audio]\n",
        "            )\n",
        "\n",
        "        return demo"
      ],
      "metadata": {
        "id": "OSDoejDHUq0B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LANCEMENT DE L'APPLICATION\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🌸 DÉMARRAGE ASSISTANT FERTILITÉ 🌸\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Initialiser l'assistant\n",
        "    assistant = SimpleMedicalAssistant()\n",
        "\n",
        "    # Créer et lancer l'interface\n",
        "    demo = assistant.create_interface()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ Interface prête! Lancement de Gradio...\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "318dcec31dd54cc18ecbffdfdc0e0b09",
            "1929fd83dd8c4fe68a6d9c58590df96e",
            "d5cbfa306bbd4cf3a084b3b8a6a6b77c",
            "95e943b3e0b04aaaa3829539c83d7375",
            "c3d277b2a0c144778b551d526943e85b",
            "f97ecf4c462f422b853fec9720376ecf",
            "0311fd9563e14aeabb2ec1ca8733f5cd",
            "8905244ce8124cd3afeafa8c6b92354e",
            "fbe01e6ce91342ac9624291de9cd2311",
            "2e472eaf0551423fa9358361051f25f2",
            "a080da3fa4e340dc96800b9f9eb9cd48",
            "58f91a9940f648f6a8fb8c368a09e355",
            "a8736518280041ccbdd46354eba117dd",
            "3b89c3f9f3c7479b972595b7ceaaad5c",
            "62bd0dde50df44cca8435b57ff48fc54",
            "c631f571d409411da4e137cbda9ab4cf",
            "830c160353b84f77a0468d67468d03c8",
            "e35f8fa08bd147899fb4f48f84bc6be3",
            "78982ea77bdc424ea82b4d4a1aeca85b",
            "5076830c79c3438d9a775ea9f53f7ac6",
            "d66b962941e34e30a377914344ee644d",
            "ea70c08530cc47babfc6bf178f97c2f5",
            "bae362ecebe447009177812f0b090e82",
            "22a11aa273a641f5b2c252e08d14cbe2",
            "a9f60d3344d44012a1507f670ebeb7ea",
            "2d3cd0a172fe4daea4236818f67d9ada",
            "33ba37518c2b4055a8400d28d2f493e6",
            "5ebe6e0397c346eb852463cf21dd84e5",
            "f643e4e2f35e42fdb16d7339aa19567a",
            "7faa97e947644b309261261e00c9dc66",
            "9bc74920fc0649ad9b3b319059ab89de",
            "1f910f8dcdd04aa1bfb316054ad9546f",
            "bfb98f805954402a893d298b7ac49f9e",
            "bbb24d4fec0d43a19679c168a54ae7f1",
            "44790d4dbd064d5bb1d56ddb48e07eea",
            "f13f0d3834524657945a4e0ea231ad8d",
            "0f6a21c9404c477fbdc7c33d2daf82f9",
            "321aa05486614b5d8727136a7ed65d58",
            "11f18864eb5746ec94ec2ed296c1e307",
            "b767abececb74f44813096dd65d45f7a",
            "d0a7650b1f424319990e655dbb08da31",
            "7c32291a3c194b0198256a3d80bbd86a",
            "1676e51aad684e70b691651de554029f",
            "d940fb2f8ddc43b28b149b327990dc30"
          ]
        },
        "id": "jhuiJMfWOd2s",
        "outputId": "33e81fe2-f30b-49c2-b0b2-66671385f96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🌸 DÉMARRAGE ASSISTANT FERTILITÉ 🌸\n",
            "============================================================\n",
            "\n",
            "🚀 Initialisation...\n",
            "  📝 STT...\n",
            "Initializing STT Module | Backend: whisper, Model: base, Language: en, Device: cpu\n",
            "Whisper model loaded successfully\n",
            "  🔊 TTS...\n",
            " gTTS prêt\n",
            "  📚 GraphRAG...\n",
            "     ✅ GraphRAG chargé\n",
            "\n",
            "✅ Prêt! (Florence2 et Qwen se chargent à la demande)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2494011037.py:301: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-2494011037.py:301: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(\n",
            "/tmp/ipython-input-2494011037.py:349: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-2494011037.py:349: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-2494011037.py:349: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "✅ Interface prête! Lancement de Gradio...\n",
            "============================================================\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://54570bd1bd54c8ec2d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://54570bd1bd54c8ec2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7c0990be6630 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Mode: Texte\n",
            "\n",
            "📸 Détection d'image, analyse...\n",
            "\n",
            "🔄 Chargement Florence2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Florence2 chargé\n",
            "✅ Image chargée: (900, 700)\n",
            "✅ Inputs préparés: ['input_ids', 'attention_mask', 'pixel_values']\n",
            "🔄 Analyse de l'image...\n",
            "✅ Génération OK, shape: torch.Size([1, 279])\n",
            "✅ Analyse terminée: 184 caractères\n",
            "🧹 Nettoyage mémoire...\n",
            "\n",
            "💭 Question: can you explaine this hormone panel?\n",
            "\n",
            "🔄 Chargement Qwen...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "318dcec31dd54cc18ecbffdfdc0e0b09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fertility_models/fertility_models/saved_models/qwen/tokenizer' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Qwen chargé, génération en cours...\n",
            "🧹 Libération Qwen...\n",
            "\n",
            "🔊 Génération audio...\n",
            "\n",
            "📢 Mode: Audio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:130: UserWarning: Performing inference on CPU when CUDA is available\n",
            "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1326/1326 [00:02<00:00, 488.76frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💭 Question: i'm 34. my amh is 1.1 ng/mL and i have pcos. should i be worried?\n",
            "\n",
            "🔄 Chargement Qwen...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58f91a9940f648f6a8fb8c368a09e355"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fertility_models/fertility_models/saved_models/qwen/tokenizer' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Qwen chargé, génération en cours...\n",
            "🧹 Libération Qwen...\n",
            "\n",
            "🔊 Génération audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 387, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7c0990be6630 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📝 Mode: Texte\n",
            "\n",
            "📸 Détection d'image, analyse...\n",
            "\n",
            "🔄 Chargement Florence2...\n",
            "✅ Florence2 chargé\n",
            "✅ Image chargée: (900, 700)\n",
            "✅ Inputs préparés: ['input_ids', 'attention_mask', 'pixel_values']\n",
            "🔄 Analyse de l'image...\n",
            "✅ Génération OK, shape: torch.Size([1, 279])\n",
            "✅ Analyse terminée: 184 caractères\n",
            "🧹 Nettoyage mémoire...\n",
            "\n",
            "💭 Question: can u explain this panel image?\n",
            "\n",
            "🔄 Chargement Qwen...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bae362ecebe447009177812f0b090e82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fertility_models/fertility_models/saved_models/qwen/tokenizer' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Qwen chargé, génération en cours...\n",
            "🧹 Libération Qwen...\n",
            "\n",
            "🔊 Génération audio...\n",
            "\n",
            "📢 Mode: Audio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:130: UserWarning: Performing inference on CPU when CUDA is available\n",
            "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1092/1092 [00:02<00:00, 426.67frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💭 Question: i'm 34. my amh is 1.1 ng/mL and i have pcos. should i be worried?\n",
            "\n",
            "🔄 Chargement Qwen...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbb24d4fec0d43a19679c168a54ae7f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/fertility_models/fertility_models/saved_models/qwen/tokenizer' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Qwen chargé, génération en cours...\n",
            "🧹 Libération Qwen...\n",
            "\n",
            "🔊 Génération audio...\n"
          ]
        }
      ]
    }
  ]
}