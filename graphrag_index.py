"""
FertilityGraphRAG - Version LOCAL (Windows/Mac/Linux)
Adapt√© depuis la version Colab
"""
import os
import warnings

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'
warnings.filterwarnings('ignore')

import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)
import zipfile
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict
import re

import networkx as nx
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma


class FertilityGraphRAG:
    """
    GraphRAG Complet pour analyse de fertilit√©
    Version locale - pas de d√©pendances Google Colab
    """

    def __init__(self, pdf_directory: str = "./fertility_docs"):
        """
        Initialise GraphRAG
        
        Args:
            pdf_directory: chemin vers le dossier contenant les PDFs
        """
        self.pdf_dir = Path(pdf_directory)
        self.pdf_dir.mkdir(parents=True, exist_ok=True)

        # Configurations
        self.chunk_size = 800
        self.chunk_overlap = 100
        self.top_k_retrieval = 5

        # Stockage
        self.documents = []
        self.chunks = []
        self.embeddings = None
        self.vector_store = None
        self.graph = nx.DiGraph()

        # Entit√©s m√©dicales sp√©cialis√©es fertilit√©
        self.medical_entities = {
            'hormones': [
                'AMH', 'FSH', 'LH', 'estradiol', 'progesterone',
                'testosterone', 'prolactin', 'thyroid', 'TSH'
            ],
            'conditions': [
                'PCOS', 'endometriosis', 'ovarian reserve', 'infertility',
                'anovulation', 'oligomenorrhea', 'amenorrhea', 'POI',
                'premature ovarian insufficiency', 'diminished ovarian reserve'
            ],
            'measurements': [
                'ng/mL', 'mIU/mL', 'pmol/L', 'IU/L', 'ng/dL',
                'follicle count', 'antral follicle', 'AFC'
            ],
            'age_groups': [
                'under 35', '35-40', 'over 40', 'advanced maternal age'
            ],
            'treatments': [
                'IVF', 'IUI', 'ovulation induction', 'letrozole',
                'clomid', 'gonadotropins', 'metformin', 'egg freezing'
            ],
            'tests': [
                'ultrasound', 'HSG', 'semen analysis', 'hormone panel',
                'ovarian reserve testing', 'AFC count'
            ]
        }

        print("üìö FertilityGraphRAG initialis√©")
        print(f"üìÅ R√©pertoire PDFs: {self.pdf_dir.absolute()}")

    # ========================================================================
    # √âTAPE 1: CHARGEMENT DES PDFs
    # ========================================================================

    def step1_load_pdfs(self) -> List:
        """Charge tous les PDFs du r√©pertoire"""
        print("\n" + "="*70)
        print("√âTAPE 1: CHARGEMENT DES PDFs")
        print("="*70)

        pdf_files = list(self.pdf_dir.glob("*.pdf"))

        if not pdf_files:
            print(f"‚ö†Ô∏è  Aucun PDF trouv√© dans {self.pdf_dir}")
            print("\nüìù Instructions:")
            print(f"   1. Cr√©ez le dossier: {self.pdf_dir}")
            print("   2. Placez vos PDFs m√©dicaux dedans")
            print("   3. Relancez le script")
            return []

        print(f"üìÑ {len(pdf_files)} PDF(s) trouv√©(s)")

        all_documents = []
        for pdf_path in pdf_files:
            try:
                print(f"\n   üìñ Chargement: {pdf_path.name}")
                loader = PyPDFLoader(str(pdf_path))
                docs = loader.load()

                # Ajouter m√©tadonn√©es
                for doc in docs:
                    doc.metadata['filename'] = pdf_path.name
                    doc.metadata['file_path'] = str(pdf_path)

                all_documents.extend(docs)
                print(f"      ‚úì {len(docs)} pages charg√©es")

            except Exception as e:
                print(f"      ‚úó Erreur: {e}")

        self.documents = all_documents
        print(f"\n‚úÖ Total: {len(all_documents)} pages charg√©es")
        return all_documents

    # ========================================================================
    # √âTAPE 2: CHUNKING
    # ========================================================================

    def step2_create_chunks(self) -> List:
        """D√©coupe les documents en chunks"""
        print("\n" + "="*70)
        print("√âTAPE 2: CHUNKING INTELLIGENT")
        print("="*70)

        if not self.documents:
            print("‚ö†Ô∏è  Aucun document charg√©")
            return []

        print(f"‚öôÔ∏è  Configuration:")
        print(f"   - Taille chunk: {self.chunk_size} caract√®res")
        print(f"   - Overlap: {self.chunk_overlap} caract√®res")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ". ", "; ", ", ", " ", ""],
            length_function=len,
        )

        print(f"\nüîÑ D√©coupage en cours...")
        chunks = text_splitter.split_documents(self.documents)

        # Enrichir m√©tadonn√©es
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = f"chunk_{i}"
            chunk.metadata['chunk_length'] = len(chunk.page_content)

        self.chunks = chunks

        print(f"‚úÖ {len(chunks)} chunks cr√©√©s")
        print(f"\nüìä Statistiques:")
        print(f"   - Taille moyenne: {sum(len(c.page_content) for c in chunks) // len(chunks)} caract√®res")
        print(f"   - Plus petit: {min(len(c.page_content) for c in chunks)} caract√®res")
        print(f"   - Plus grand: {max(len(c.page_content) for c in chunks)} caract√®res")

        if chunks:
            print(f"\nüí° Exemple de chunk:")
            print(f"   {chunks[0].page_content[:200]}...")

        return chunks

    # ========================================================================
    # √âTAPE 3: EMBEDDINGS
    # ========================================================================

    def step3_create_embeddings(self):
        """Cr√©e le mod√®le d'embeddings"""
        print("\n" + "="*70)
        print("√âTAPE 3: CR√âATION DES EMBEDDINGS")
        print("="*70)

        print("üîÑ Chargement du mod√®le d'embeddings...")
        print("   Mod√®le: sentence-transformers/all-MiniLM-L6-v2")
        print("   - Taille: ~90MB")
        print("   - Dimension: 384")
        print("   - Optimis√© pour CPU")

        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        print("‚úÖ Mod√®le d'embeddings charg√©")

        # Test
        test_text = "AMH level of 1.1 ng/mL in PCOS patient"
        test_embedding = self.embeddings.embed_query(test_text)
        print(f"\nüß™ Test d'embedding:")
        print(f"   Texte: '{test_text}'")
        print(f"   Dimension: {len(test_embedding)}")

        return self.embeddings

    # ========================================================================
    # √âTAPE 4: KNOWLEDGE GRAPH
    # ========================================================================

    def step4_build_knowledge_graph(self):
        """Construit le knowledge graph"""
        print("\n" + "="*70)
        print("√âTAPE 4: CONSTRUCTION DU KNOWLEDGE GRAPH")
        print("="*70)

        if not self.chunks:
            print("‚ö†Ô∏è  Aucun chunk disponible")
            return

        print("üîÑ Construction du graphe...")

        entity_counts = defaultdict(int)

        for i, chunk in enumerate(self.chunks):
            chunk_id = chunk.metadata['chunk_id']
            text = chunk.page_content.lower()

            # Ajouter n≈ìud chunk
            self.graph.add_node(
                chunk_id,
                type='chunk',
                text=chunk.page_content[:300],
                full_text=chunk.page_content,
                source=chunk.metadata.get('filename', 'unknown'),
                page=chunk.metadata.get('page', 0),
                chunk_length=len(chunk.page_content)
            )

            # Extraire entit√©s
            chunk_entities = self.extract_entities_advanced(text)

            for category, entity_list in chunk_entities.items():
                for entity in entity_list:
                    entity_node = f"entity_{category}_{entity.lower().replace(' ', '_')}"

                    entity_counts[entity] += 1

                    if not self.graph.has_node(entity_node):
                        self.graph.add_node(
                            entity_node,
                            type='entity',
                            category=category,
                            name=entity,
                            occurrences=1
                        )
                    else:
                        self.graph.nodes[entity_node]['occurrences'] += 1

                    self.graph.add_edge(
                        chunk_id,
                        entity_node,
                        relation='mentions',
                        weight=1.0
                    )

            if (i + 1) % 50 == 0:
                print(f"   üìà Trait√©: {i + 1}/{len(self.chunks)} chunks")

        # Relations entre entit√©s
        self._create_entity_cooccurrence_edges()

        print(f"\n‚úÖ Knowledge Graph construit")
        print(f"\nüìä Statistiques:")
        print(f"   - N≈ìuds totaux: {self.graph.number_of_nodes()}")
        print(f"   - Ar√™tes totales: {self.graph.number_of_edges()}")
        print(f"   - N≈ìuds 'chunk': {sum(1 for n in self.graph.nodes() if self.graph.nodes[n].get('type') == 'chunk')}")
        print(f"   - N≈ìuds 'entity': {sum(1 for n in self.graph.nodes() if self.graph.nodes[n].get('type') == 'entity')}")

        print(f"\nüèÜ Top 5 entit√©s:")
        top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        for entity, count in top_entities:
            print(f"   - {entity}: {count} fois")

        return self.graph

    def extract_entities_advanced(self, text: str) -> Dict[str, List[str]]:
        """Extraction d'entit√©s avec regex"""
        found_entities = defaultdict(list)

        for category, entities in self.medical_entities.items():
            for entity in entities:
                pattern = r'\b' + re.escape(entity.lower()) + r'\b'
                if re.search(pattern, text):
                    found_entities[category].append(entity)

        # Valeurs num√©riques
        numeric_patterns = [
            (r'(\d+\.?\d*)\s*(ng/ml|ng/dl|miu/ml|iu/l|pmol/l)', 'measurements'),
            (r'(\d+)\s*follicles?', 'measurements'),
        ]

        for pattern, category in numeric_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                value = match.group(0)
                if value not in found_entities[category]:
                    found_entities[category].append(value)

        return dict(found_entities)

    def _create_entity_cooccurrence_edges(self):
        """Cr√©e relations entre entit√©s co-occurrentes"""
        print("\n   üîó Cr√©ation des relations entre entit√©s...")

        entity_nodes = [n for n in self.graph.nodes() if self.graph.nodes[n].get('type') == 'entity']

        cooccurrence_count = 0
        for entity1 in entity_nodes:
            chunks_with_entity1 = list(self.graph.predecessors(entity1))

            for entity2 in entity_nodes:
                if entity1 >= entity2:
                    continue

                chunks_with_entity2 = list(self.graph.predecessors(entity2))
                common_chunks = set(chunks_with_entity1) & set(chunks_with_entity2)

                if common_chunks:
                    weight = len(common_chunks) / max(len(chunks_with_entity1), len(chunks_with_entity2))
                    self.graph.add_edge(
                        entity1,
                        entity2,
                        relation='co_occurs_with',
                        weight=weight,
                        common_chunks=len(common_chunks)
                    )
                    cooccurrence_count += 1

        print(f"      ‚úì {cooccurrence_count} relations cr√©√©es")

    # ========================================================================
    # √âTAPE 5: VECTOR STORE
    # ========================================================================

    def step5_build_vector_store(self):
        """Construit le vector store"""
        print("\n" + "="*70)
        print("√âTAPE 5: CONSTRUCTION DU VECTOR STORE")
        print("="*70)

        if not self.chunks:
            print("‚ö†Ô∏è  Aucun chunk disponible")
            return

        if not self.embeddings:
            print("‚ö†Ô∏è  Embeddings non cr√©√©s")
            return

        print("üîÑ Vectorisation des chunks...")
        print(f"   - {len(self.chunks)} chunks √† vectoriser")
        print("   - Cela peut prendre 1-2 minutes...")

        # Supprimer ancienne DB
        chroma_dir = Path("./chroma_db")
        if chroma_dir.exists():
            import shutil
            shutil.rmtree(chroma_dir)
            print("   üóëÔ∏è  Ancienne base supprim√©e")

        # Cr√©er vector store
        self.vector_store = Chroma.from_documents(
            documents=self.chunks,
            embedding=self.embeddings,
            persist_directory="./chroma_db",
            collection_name="fertility_collection"
        )

        print(f"‚úÖ Vector store cr√©√©")
        print(f"   - Collection: fertility_collection")
        print(f"   - R√©pertoire: ./chroma_db")
        print(f"   - {len(self.chunks)} vecteurs stock√©s")

        # Test
        print("\nüß™ Test de recherche:")
        test_query = "What is a normal AMH level?"
        results = self.vector_store.similarity_search(test_query, k=2)
        print(f"   Query: '{test_query}'")
        print(f"   R√©sultats: {len(results)}")
        if results:
            print(f"   Premier: {results[0].page_content[:100]}...")

        return self.vector_store

    # ========================================================================
    # √âTAPE 6: RETRIEVAL HYBRIDE
    # ========================================================================

    def step6_retrieve(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """R√©cup√©ration hybride (Vector + Graph)"""
        print(f"\nüîç Query: '{query}'")

        if not self.vector_store:
            return {"error": "Vector store non initialis√©"}

        # 1. VECTOR SEARCH
        print("\n   1Ô∏è‚É£ Vector Search...")
        vector_results = self.vector_store.similarity_search_with_score(query, k=top_k)
        print(f"      ‚úì {len(vector_results)} r√©sultats")

        # 2. GRAPH SEARCH (seulement si le graph existe)
        graph_chunks = []
        entity_info = []
        query_entities = {}
        
        if self.graph.number_of_nodes() > 0:
            print("   2Ô∏è‚É£ Graph Search...")
            query_entities = self.extract_entities_advanced(query.lower())

            for category, entities in query_entities.items():
                for entity in entities:
                    entity_node = f"entity_{category}_{entity.lower().replace(' ', '_')}"

                    if self.graph.has_node(entity_node):
                        node_data = self.graph.nodes[entity_node]
                        entity_info.append({
                            'entity': entity,
                            'category': category,
                            'occurrences': node_data.get('occurrences', 0)
                        })

                        connected_chunks = list(self.graph.predecessors(entity_node))

                        for chunk_id in connected_chunks[:3]:
                            chunk_text = self.graph.nodes[chunk_id].get('full_text', '')
                            if chunk_text:
                                graph_chunks.append({
                                    'chunk_id': chunk_id,
                                    'text': chunk_text,
                                    'entity': entity,
                                    'source': self.graph.nodes[chunk_id].get('source', 'unknown')
                                })

            print(f"      ‚úì {len(graph_chunks)} chunks via graphe")
        else:
            print("   2Ô∏è‚É£ Graph Search: SKIPPED (cache mode - vector search only)")

        return {
            'query': query,
            'vector_results': [
                {
                    'text': doc.page_content,
                    'metadata': doc.metadata,
                    'score': float(score)
                }
                for doc, score in vector_results
            ],
            'graph_results': graph_chunks[:5],
            'detected_entities': query_entities,
            'entity_info': entity_info
        }

    def format_context_for_llm(self, retrieval_results: Dict[str, Any]) -> str:
        """Formate le contexte pour le LLM"""
        context_parts = []

        context_parts.append("=== MEDICAL CONTEXT ===\n")
        context_parts.append(f"Question: {retrieval_results['query']}\n")

        # Entit√©s
        if retrieval_results['detected_entities']:
            context_parts.append("\n--- DETECTED ENTITIES ---")
            for category, entities in retrieval_results['detected_entities'].items():
                context_parts.append(f"{category.upper()}: {', '.join(entities)}")

        # Vector results
        context_parts.append("\n--- RELEVANT INFORMATION (Vector Search) ---")
        for i, result in enumerate(retrieval_results['vector_results'][:3], 1):
            source = result['metadata'].get('filename', 'unknown')
            page = result['metadata'].get('page', '?')
            score = result['score']
            context_parts.append(
                f"\n[{i}] Source: {source} (page {page}) | Score: {score:.3f}\n"
                f"{result['text']}"
            )

        # Graph results
        if retrieval_results['graph_results']:
            context_parts.append("\n--- RELATED INFORMATION (Graph Search) ---")
            for i, result in enumerate(retrieval_results['graph_results'][:2], 1):
                context_parts.append(
                    f"\n[{i}] Entity: {result['entity']} | Source: {result['source']}\n"
                    f"{result['text'][:400]}..."
                )

        context_parts.append("\n=== END CONTEXT ===")

        return "\n".join(context_parts)

    # ========================================================================
    # PIPELINE COMPLET
    # ========================================================================

    def run_full_pipeline(self):
        """Ex√©cute le pipeline complet"""
        print("\n" + "üöÄ"*35)
        print("PIPELINE GRAPHRAG COMPLET")
        print("üöÄ"*35)

        docs = self.step1_load_pdfs()
        if not docs:
            return False

        self.step2_create_chunks()
        self.step3_create_embeddings()
        self.step4_build_knowledge_graph()
        self.step5_build_vector_store()

        print("\n" + "="*70)
        print("‚úÖ PIPELINE TERMIN√â")
        print("="*70)

        return True


# ========================================================================
# MAIN - LANCEMENT DU SCRIPT
# ========================================================================

if __name__ == "__main__":
    print("üöÄ D√©marrage de FertilityGraphRAG...\n")
    
    # Cr√©er l'instance
    graphrag = FertilityGraphRAG(pdf_directory="./fertility_docs")
    
    # Lancer le pipeline complet
    success = graphrag.run_full_pipeline()
    
    if success:
        print("\n" + "="*70)
        print("‚úÖ SYST√àME PR√äT - TEST DE RECHERCHE")
        print("="*70)
        
        # Test
        test_query = "What is AMH?"
        print(f"\nüîç Test Query: '{test_query}'")
        results = graphrag.step6_retrieve(test_query, top_k=3)
        
        print(f"\nüìä R√©sultats:")
        print(f"   - Vector results: {len(results.get('vector_results', []))}")
        print(f"   - Graph results: {len(results.get('graph_results', []))}")
        print(f"   - Entit√©s d√©tect√©es: {results.get('detected_entities', {})}")
        
        # Afficher le contexte format√©
        print("\nüìÑ Contexte format√© pour LLM:")
        context = graphrag.format_context_for_llm(results)
        print(context[:800] + "...\n")
    else:
        print("\n‚ùå ERREUR: Aucun PDF trouv√©")
        print("üìù Placez vos PDFs m√©dicaux dans ./fertility_docs")
